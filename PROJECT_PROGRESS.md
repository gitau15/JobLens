# ğŸš€ JobLens - Development Progress

**Project**: JobLens - Autonomous AI Agent for Semantic CV-Job Matching and Personalized Daily Job Recommendation  
**Last Updated**: January 13, 2026

---

## ğŸ“‹ Project Objectives

1. **Data Acquisition Module**: Automatically scrape/retrieve job postings from multiple sources with deduplication
2. **NLP-Based CV Parsing & Semantic Matching**: Extract skills/experience from CVs and compute similarity with jobs
3. **Automated Job-Recommendation Workflow**: Rank jobs, generate daily reports, deliver via email/Telegram
4. **Scalable User Interface**: CV upload, job viewing, preference management
5. **System Evaluation**: Measure accuracy, ranking quality, user satisfaction, processing efficiency

---

## âœ… Completed Components

### 1. **Scraper Service** (`scraper_service/`)
**Status**: âœ… **Production Ready**

- **Tech Stack**: FastAPI, Pydantic, httpx, BeautifulSoup4
- **Features**:
  - `/health` endpoint for monitoring
  - `/scrape` endpoint with request/response validation
  - Two live job sources integrated:
    - **Remotive API** (remote jobs worldwide)
    - **Arbeitnow API** (general job board)
  - Automatic URL-based deduplication (`job_url_hash`)
  - Type-safe models (`JobModel`, `ScrapeRequest`, `ScrapeResponse`)
  - HTML-to-text conversion for job descriptions
  
- **Files**:
  - `main.py` - FastAPI app & scraping dispatcher
  - `models.py` - Pydantic data models
  - `scrapers/remotive.py` - Remotive API scraper
  - `scrapers/arbeitnow.py` - Arbeitnow API scraper
  - `requirements.txt` - Dependencies

- **Testing**: âœ… Tested via Swagger UI at `http://localhost:8000/docs`

---

### 2. **Embedder Service** (`embedder_service/`)
**Status**: âœ… **Built & Debugged**

- **Tech Stack**: FastAPI, sentence-transformers, ChromaDB, Pydantic
- **Features**:
  - `/health` endpoint
  - `/embed/jobs` - Batch embed jobs into ChromaDB with metadata
  - `/embed/query` - Embed CV text or search queries
  - Uses `sentence-transformers/all-MiniLM-L6-v2` (CPU-friendly, open source)
  - ChromaDB persistent storage at `chroma_data/`
  
- **Files**:
  - `main.py` - FastAPI app with embedding endpoints
  - `models.py` - Pydantic models (`JobToEmbed`, `EmbedJobsRequest`, etc.)
  - `requirements.txt` - Dependencies including sentence-transformers & chromadb

- **Environment**: 
  - âœ… Python 3.11 venv configured (resolved Python 3.13 compatibility issues)
  - âœ… All dependencies installed successfully

- **Port**: Runs on port 8002 (8001 had conflicts)

---

### 3. **Supabase Database Setup** (`supabase/`)
**Status**: âœ… **Completed**

- **Schema Design**: Complete database schema created with all required tables:
  - `raw_jobs`: All scraped jobs with embedding status tracking
  - `users`: User profiles (linked to Supabase auth)
  - `cvs`: Uploaded/parsed CV data per user
  - `user_preferences`: Job preferences (role, location, remote, salary)
  - `recommendations`: Daily job matches with scores and feedback
- **Security**: Row Level Security (RLS) policies implemented for data privacy
- **Performance**: Proper indexes created for optimized queries
- **Files**:
  - `schema.sql` - Complete database schema with tables, indexes, and RLS policies
  - `supabase_config.md` - Setup instructions and configuration guide

---

### 4. **n8n Workflow Orchestration** (`n8n_workflows/`)
**Status**: âœ… **Completed**

- **Tech Stack**: n8n via Docker container
- **Features**:
  - **Job Ingestion Pipeline**: Runs every 6 hours, scrapes jobs from sources, stores in Supabase, and sends to embedder service
  - **Daily Recommendation Pipeline**: Runs daily at 8:00 AM, matches CVs to jobs, stores recommendations, and sends notifications
  - Proper error handling and logging
- **Files**:
  - `job_ingestion_pipeline.json` - Complete workflow for job ingestion
  - `daily_recommendation_pipeline.json` - Complete workflow for daily recommendations
- **Port**: Runs on port 5678 (Docker container)

---

### 5. **Matcher Service** (`matcher_service/`)
**Status**: âœ… **Completed**

- **Tech Stack**: FastAPI, ChromaDB, Pydantic
- **Features**:
  - `/health` endpoint for monitoring
  - `/match` endpoint for semantic CV-job matching
  - `/match-with-rerank` endpoint with business logic re-ranking
  - Integration with ChromaDB for vector similarity search
  - Support for business filters (location, remote work, etc.)
- **Files**:
  - `main.py` - FastAPI app with matching endpoints
  - `models.py` - Pydantic models for request/response
  - `requirements.txt` - Dependencies
  - `README.md` - Documentation
- **Port**: Runs on port 8001

---

### 6. **Frontend Application** (`frontend/`)
**Status**: âœ… **Completed**

- **Tech Stack**: React, TypeScript, Vite, Supabase
- **Features**:
  - Complete authentication system (login/signup)
  - Dashboard with user statistics
  - Job recommendations with filtering
  - Profile management
  - Responsive design
- **Pages**:
  - Home page with feature highlights
  - Authentication pages (login/signup)
  - Dashboard with navigation
  - Job listings with match scores
  - Profile management
- **Files**:
  - Complete component structure with contexts, pages, and components
  - TypeScript configuration
  - Vite build configuration
  - Comprehensive CSS styling
  - Environment configuration
  - Documentation

---

## ğŸ”„ In Progress / Blocked

### Environment Setup
- âœ… Python 3.11 virtual environment at `d:\\FinalYear\\.venv`
- âœ… Separate venv for `embedder_service/.venv` (fallback)
- âš ï¸ **Known Issue**: Port 8001 conflict (use 8002 for embedder) - RESOLVED: Matcher service now uses port 8001

---

## ğŸ”´ Not Started (Next Steps)

### 1. **CV Parser Module**
- [ ] PDF/DOCX upload support (`PyPDF2`, `python-docx`)
- [ ] Extract text from uploaded files
- [ ] Optional: Use spaCy or regex to extract skills, experience, education
- [ ] Store parsed data in Supabase `cvs` table

### 2. **Notification System**
- [ ] Email notifications:
  - Configure SMTP (Gmail or Brevo/Resend free tier)
  - Design email template with top jobs
- [ ] Telegram notifications:
  - Create Telegram bot
  - Store user `telegram_chat_id` in Supabase
  - Send concise job list or link to UI

### 3. **Frontend Integration**
- [ ] Connect frontend to backend services
- [ ] Implement CV upload functionality
- [ ] Connect to Supabase database
- [ ] Integrate with matcher service for job recommendations
- [ ] Add feedback collection (like/dislike buttons)

### 4. **Evaluation & Metrics**
- [ ] Log all recommendations (job IDs, scores, timestamps)
- [ ] Collect user feedback (like/dislike buttons in UI)
- [ ] Implement metrics calculation:
  - Precision@k, NDCG for ranking quality
  - Click-through rate, application rate
  - Processing time per batch
- [ ] Generate evaluation report for project defense

---

## ğŸ› ï¸ Technical Stack Summary

| Component | Technology |
|-----------|------------|
| **Job Scraping** | FastAPI, httpx, BeautifulSoup4 |
| **Embeddings** | sentence-transformers (all-MiniLM-L6-v2) |
| **Vector DB** | ChromaDB (persistent local) |
| **Database** | Supabase (PostgreSQL, free tier) |
| **Orchestration** | n8n (self-hosted or cloud free tier) |
| **CV Parsing** | PyPDF2, python-docx, spaCy (optional) |
| **Notifications** | Email (SMTP), Telegram Bot API |
| **Frontend** | React (Next.js/Vite), Supabase Auth, Vercel |
| **Language** | Python 3.11 |

---

## ğŸ“ Project Structure

```
d:\\FinalYear\\
â”œâ”€â”€ scraper_service/
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ remotive.py      âœ… Remotive API scraper
â”‚   â”‚   â””â”€â”€ arbeitnow.py     âœ… Arbeitnow API scraper
â”‚   â”œâ”€â”€ main.py              âœ… FastAPI app
â”‚   â”œâ”€â”€ models.py            âœ… Pydantic models
â”‚   â””â”€â”€ requirements.txt     âœ…
â”‚
â”œâ”€â”€ embedder_service/
â”‚   â”œâ”€â”€ main.py              âœ… FastAPI app with ChromaDB
â”‚   â”œâ”€â”€ models.py            âœ… Pydantic models
â”‚   â”œâ”€â”€ requirements.txt     âœ…
â”‚   â””â”€â”€ chroma_data/         (created at runtime)
â”‚
â”œâ”€â”€ matcher_service/         âœ… Complete
â”‚   â”œâ”€â”€ main.py              âœ… FastAPI app with matching endpoints
â”‚   â”œâ”€â”€ models.py            âœ… Pydantic models
â”‚   â”œâ”€â”€ requirements.txt     âœ… Dependencies
â”‚   â””â”€â”€ README.md            âœ… Documentation
â”‚
â”œâ”€â”€ supabase/                âœ… Complete
â”‚   â”œâ”€â”€ schema.sql           âœ… Database schema with tables and RLS
â”‚   â””â”€â”€ supabase_config.md   âœ… Setup instructions
â”‚
â”œâ”€â”€ n8n_workflows/           âœ… Complete
â”‚   â”œâ”€â”€ job_ingestion_pipeline.json        âœ… Job ingestion workflow
â”‚   â””â”€â”€ daily_recommendation_pipeline.json âœ… Daily recommendation workflow
â”‚
â”œâ”€â”€ frontend/                âœ… Complete
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/      âœ… Reusable UI components
â”‚   â”‚   â”œâ”€â”€ contexts/        âœ… React context providers
â”‚   â”‚   â”œâ”€â”€ pages/           âœ… Route components (Home, Login, Dashboard, etc.)
â”‚   â”‚   â”œâ”€â”€ App.tsx          âœ… Main application component
â”‚   â”‚   â”œâ”€â”€ main.tsx         âœ… Entry point
â”‚   â”‚   â””â”€â”€ index.css        âœ… Global styles
â”‚   â”œâ”€â”€ package.json         âœ… Dependencies
â”‚   â”œâ”€â”€ vite.config.ts       âœ… Vite configuration
â”‚   â”œâ”€â”€ tsconfig.json        âœ… TypeScript configuration
â”‚   â””â”€â”€ README.md            âœ… Documentation
â”‚
â””â”€â”€ .venv/                   âœ… Python 3.11 environment
```

---

## ğŸ› Known Issues & Solutions

### Issue 1: Python 3.13 Dependency Compilation Errors
**Problem**: `numpy`, `chromadb` tried to build from source, requiring C++ build tools  
**Solution**: âœ… Switched to Python 3.11; all dependencies installed as precompiled wheels

### Issue 2: Port 8001 Already in Use
**Problem**: First uvicorn instance still running in background  
**Solution**: âœ… Use port 8002 for embedder service or kill process with `taskkill /PID <PID> /F`

### Issue 3: ChromaDB Not Found in Embedder Service
**Problem**: Local `.venv` inside `embedder_service/` missing dependencies  
**Solution**: Use main project venv at `d:\FinalYear\.venv` or reinstall in local venv

---

## ğŸ“Š Progress Estimate

| Phase | Completion |
|-------|------------|
| **Phase 1: Data Pipeline** (Scraping + Embedding) | âœ… **100%** |
| **Phase 2: Database & Orchestration** (Supabase + n8n) | âœ… **100%** |
| **Phase 3: Matching & Ranking** (Matcher service) | âœ… **100%** |
| **Phase 4: Frontend** (React + Auth) | âœ… **100%** |
| **Phase 5: Notifications** (Email + Telegram) | ğŸ”´ **0%** |
| **Phase 6: CV Parser** (PDF/DOCX processing) | ğŸ”´ **0%** |
| **Phase 7: Evaluation** (Metrics + Report) | ğŸ”´ **0%** |

**Overall Project Completion**: ~70-75%

---

## ğŸ¯ Recommended Next Session Plan

1. **Implement CV Parser Module** (60-90 min)
   - Add PDF/DOCX upload functionality to frontend
   - Implement file parsing with PyPDF2/python-docx
   - Extract text and key information (skills, experience, education)
   - Connect to Supabase `cvs` table

2. **Frontend Integration** (45-60 min)
   - Connect frontend to backend services (matcher, embedder)
   - Implement job recommendation display
   - Add feedback collection (like/dislike buttons)

3. **Notification System Setup** (30-45 min)
   - Configure email notifications (SMTP setup)
   - Set up Telegram bot for notifications
   - Test notification workflows with n8n

---

## ğŸ“ Notes

- All services use **free/open-source tools** to meet budget constraint
- **Sentence-transformers model** downloads ~80MB on first run (one-time)
- For production deployment, consider:
  - Dockerizing all services
  - Using cloud-hosted ChromaDB (Chroma Cloud free tier) or vector DB alternative
  - Rate limiting on scraper to respect API terms

---

## ğŸ”— Useful Commands

### Run Scraper Service
```powershell
cd d:\FinalYear\scraper_service
. ..\.venv\Scripts\Activate.ps1
uvicorn main:app --reload --port 8000
```
Access: `http://localhost:8000/docs`

### Run Embedder Service
```powershell
cd d:\FinalYear\embedder_service
. ..\.venv\Scripts\Activate.ps1
uvicorn main:app --port 8002
```
Access: `http://localhost:8002/docs`

### Test Scraper
```powershell
Invoke-RestMethod -Uri "http://localhost:8000/scrape" -Method Post -ContentType "application/json" -Body '{"source_name":"remotive","max_jobs":5}'
```

---

**End of Progress Report**
